import os
import asyncio
import json
from typing import Annotated, Sequence, TypedDict, List, Dict, Any
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.async_configs import LLMConfig
from langchain_core.messages import BaseMessage, ToolMessage, SystemMessage, HumanMessage
from langchain_core.tools import tool
from langgraph.graph.message import add_messages
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
import csv
from langchain_community.chat_models import ChatTongyi
from langchain_community.llms import Tongyi
from langchain_community.chat_models.tongyi import ChatTongyi
import re



load_dotenv()

# çŠ¶æ€å®šä¹‰
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    news_data: List[Dict[str, Any]]
    factor_data: List[Dict[str, Any]]
    url: str

# Pydantic models
class NewsURL(BaseModel):
    url: str = Field(..., description="æå–æ–°é—»å‘å¸ƒçš„é“¾æ¥,åªè¿”å›é“¾æ¥,ä¸è¦è¿”å›å…¶ä»–å†…å®¹.")

class NewsContent(BaseModel):
    news_time: str = Field(..., description="æ–°é—»å‘å¸ƒçš„æ—¶é—´,åŒ…æ‹¬æ—¥æœŸã€æ—¶é—´,å¦‚ 2023å¹´10æœˆ5æ—¥ 14:30.")
    news_title: str = Field(..., description="æ–°é—»åŸæ–‡çš„å®Œæ•´æ ‡é¢˜ï¼Œç¦æ­¢ä¿®æ”¹æˆ–ç¼©å†™.")
    news_text: str = Field(..., description="æ–°é—»çš„å…¨éƒ¨æ­£æ–‡å†…å®¹.")
    company_involved: str = Field(..., description="æ–°é—»ä¸­æ‰€æœ‰å…³è”çš„ä¸Šå¸‚å…¬å¸éƒ½è¦åˆ—å‡ºï¼Œå¤šæ¡å…¬å¸å¤šæ¡è®°å½•ï¼Œä½¿ç”¨å…¶æ³•å®šæ³¨å†Œå…¨ç§°.")
    stock_code: str = Field(..., description="ä¸æ¶‰åŠå…¬å¸å¯¹åº”çš„å”¯ä¸€å®˜æ–¹è¯åˆ¸ä»£ç .")
    stock_short_name: str = Field(..., description="ä¸æ¶‰åŠå…¬å¸å¯¹åº”çš„äº¤æ˜“æ‰€æ ‡å‡†åŒ–ç®€ç§°.")

class NewsImpact(BaseModel):
    company_name: str = Field(..., description="å…¬å¸åç§°")
    stock_code: str = Field(..., description="è‚¡ç¥¨ä»£ç ")
    stock_short_name: str = Field(..., description="è‚¡ç¥¨ç®€ç§°")
    news_time: str = Field(..., description="æ–°é—»æ—¶é—´")
    news_title: str = Field(..., description="æ–°é—»æ ‡é¢˜")
    impact_direction: int = Field(..., description="å½±å“æ–¹å‘: 1(æ­£é¢)/-1(è´Ÿé¢)/0(ä¸­æ€§)")
    news_summary: str = Field(..., description="æ–°é—»æ‘˜è¦")

class NewsInput(BaseModel):
    news_data: List[Dict[str, Any]] = Field(..., description="æ–°é—»æ•°æ®åˆ—è¡¨")

class URLInput(BaseModel):
    user_query: str = Field(..., description="ç”¨æˆ·çš„æŸ¥è¯¢æˆ–éœ€æ±‚æè¿°")

# Tools
@tool
async def crawl_and_save_news(url: str) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨ crawl4ai ä»ç»™å®š URL æŠ“å–æ–°é—»å¹¶æå–å†…å®¹ï¼ŒåŒæ—¶ä¿å­˜åŸå§‹æ–°é—»æ•°æ®ã€‚
    
    å‚æ•°
        urlï¼š è¦æŠ“å–æ–°é—»çš„ URLã€‚é»˜è®¤ä¸º eastmoney quick newsã€‚
        
    è¿”å›å€¼
        åŒ…å«æå–æ–°é—»å†…å®¹çš„å­—å…¸åˆ—è¡¨ã€‚
    """
    async with AsyncWebCrawler() as crawler:
        # First layer: Extract news links
        links_result = await crawler.arun(
            url=url,
            config=CrawlerRunConfig(
                word_count_threshold=1,
                extraction_strategy=LLMExtractionStrategy(
                    llm_config=LLMConfig(
                        provider="deepseek/deepseek-chat",
                        api_token=os.getenv("DEEPSEEK_API_KEY"),
                    ),
                    schema=NewsURL.model_json_schema(),
                    extraction_type="schema",
                    instruction="""è¯·æå–æ‰€æœ‰æ–°é—»ä¿¡æ¯çš„urlã€‚ç¤ºä¾‹ï¼š
                    14:11[**ã€ä¸ºäº†"å‡ºæµ·"å’Œ"è¿˜è´·" é”¦æ±Ÿé…’åº—æ‹Ÿå¯åŠ¨æ¸¯è‚¡IPOã€‘** 
                    ä»é”¦æ±Ÿé…’åº—æ¥çœ‹ï¼Œå…¶å‡ºæµ·å¸ƒå±€å·²è¿›å…¥å®è´¨é˜¶æ®µï¼Œè€Œä¸œå—äºšå¸‚åœºæˆä¸ºå‡ºæµ·çš„é‡è¦è½ç‚¹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå…¬å‘Šè¿˜æåˆ°å‹Ÿèµ„å°†ç”¨äºå¿è¿˜é“¶è¡Œè´·æ¬¾ã€‚
                    [ç‚¹å‡»æŸ¥çœ‹å…¨æ–‡]](https://finance.eastmoney.com/a/202506053422839540.html).æˆ‘ä»¬éœ€è¦è¿”å›çš„åªæœ‰https://finance.eastmoney.com/a/202506053422839540.html"""
                ),
                cache_mode=CacheMode.BYPASS
            )
        )
        
        if not links_result.success or not links_result.extracted_content:
            print("æœªæ‰¾åˆ°ä»»ä½•æ–°é—»é“¾æ¥")
            return []
            
        content = json.loads(links_result.extracted_content) if isinstance(links_result.extracted_content, str) else links_result.extracted_content
        news_links = [item['url'] for item in content if isinstance(item, dict) and item.get('url')]
        
        total_links = len(news_links)
        print(f"\næ€»å…±æ‰¾åˆ° {total_links} æ¡æ–°é—»é“¾æ¥")
        print("å¼€å§‹æå–æ–°é—»å†…å®¹")        
        # # é™åˆ¶å¤„ç†çš„é“¾æ¥æ•°é‡ä¸º20æ¡
        # news_links = news_links[:2]
        # print(f"å°†å¤„ç†å‰ 20 æ¡æ–°é—»\n")
        
        # Second layer: Extract news content
        all_news = []
        for i, news_url in enumerate(news_links, 1):
            print(f"æ­£åœ¨å¤„ç†ç¬¬ {i}/{len(news_links)} æ¡æ–°é—»: {news_url}")
            content_result = await crawler.arun(
                url=news_url,
                config=CrawlerRunConfig(
                    word_count_threshold=1,
                    extraction_strategy=LLMExtractionStrategy(
                        llm_config=LLMConfig(
                            provider="deepseek/deepseek-chat",
                            api_token=os.getenv("DEEPSEEK_API_KEY"),
                        ),
                        schema=NewsContent.model_json_schema(),
                        extraction_type="schema",
                        instruction="""
                    è¯·ä»æ–°é—»é¡µé¢æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
                                1. news_time: æ–°é—»å‘å¸ƒçš„å…·ä½“æ—¶é—´ï¼ˆæ ¼å¼ï¼šYYYYå¹´MMæœˆDDæ—¥ HH:mmï¼‰
                                2. news_title: æ–°é—»çš„å®Œæ•´æ ‡é¢˜
                                3. news_text: æ–°é—»çš„å®Œæ•´æ­£æ–‡å†…å®¹
                                4. company_involved: æ–°é—»ä¸­æåˆ°çš„ä¸Šå¸‚å…¬å¸å…¨ç§°
                                5. stock_code: å¯¹åº”çš„è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ï¼š000001.SZï¼‰
                                6. stock_short_name: å…¬å¸åœ¨äº¤æ˜“æ‰€çš„ç®€ç§°

                                æ³¨æ„ï¼š
                                - å¦‚æœæ–°é—»æ¶‰åŠå¤šå®¶å…¬å¸ï¼Œè¯·åˆ†åˆ«åˆ›å»ºå¤šæ¡è®°å½•
                                - æ¯æ¡è®°å½•çš„news_timeã€news_titleå’Œnews_textä¿æŒç›¸åŒ
                                - company_involvedã€stock_codeå’Œstock_short_nameå¯¹åº”æ¯å®¶å…¬å¸çš„å…·ä½“ä¿¡æ¯
                                - å¦‚æœæ‰¾ä¸åˆ°æŸä¸ªå­—æ®µçš„ä¿¡æ¯ï¼Œè¯·è¿”å›ç©ºå­—ç¬¦ä¸²""
                                - è¿”å›æ ¼å¼å¿…é¡»æ˜¯JSONæ•°ç»„"""
                    ),
                    cache_mode=CacheMode.BYPASS,
                )
            )
            
            if content_result.success and content_result.extracted_content:
                news_content = json.loads(content_result.extracted_content) if isinstance(content_result.extracted_content, str) else content_result.extracted_content

                if isinstance(news_content, list):
                    all_news.extend(news_content)
                else:
                    all_news.append(news_content)
                print("âˆš æå–æˆåŠŸ")
                print(news_content)
            else:
                print("Ã— æå–å¤±è´¥")

        
        print(f"\næ–°é—»å¤„ç†å®Œæˆ:")
        print(f"- æ€»é“¾æ¥æ•°: {total_links}")
        print(f"- å¤„ç†é“¾æ¥æ•°: {len(news_links)}")
        print(f"- æå–çš„å…¬å¸è®°å½•æ•°: {len(all_news)}")
        
        # Save raw news data
        if all_news:
            fieldnames = ['æ—¶é—´', 'æ ‡é¢˜', 'æ­£æ–‡', 'æ¶‰åŠå…¬å¸', 'è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨ç®€ç§°']
            field_mapping = {
                'news_time': 'æ—¶é—´',
                'news_title': 'æ ‡é¢˜',
                'news_text': 'æ­£æ–‡',
                'company_involved': 'æ¶‰åŠå…¬å¸',
                'stock_code': 'è‚¡ç¥¨ä»£ç ',
                'stock_short_name': 'è‚¡ç¥¨ç®€ç§°'
            }
            try:
                with open('news.csv', 'w', newline='', encoding='utf-8') as csvfile:
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                    for item in all_news:
                        row = {field_mapping[k]: item.get(k, "") for k in field_mapping}
                        writer.writerow(row)
                print(f"- åŸå§‹æ–°é—»æ•°æ®å·²ä¿å­˜åˆ° news.csv")
            except Exception as e:
                print(f"ä¿å­˜CSVæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
                    
        return all_news

@tool
def analyze_news_impact(input_data: NewsInput) -> List[Dict[str, Any]]:
    """åˆ†ææ–°é—»å¯¹å…¬å¸çš„å½±å“"""
    llm = ChatTongyi(model="qwen-plus",api_key=os.getenv("DASHSCOPE_API_KEY"))
    
    factor_data = []
    for news in input_data.news_data:
        prompt = f"""
        è¯·åˆ†æä»¥ä¸‹æ–°é—»å¯¹å…¬å¸çš„å½±å“ï¼š
        
        æ–°é—»æ ‡é¢˜ï¼š{news['news_title']}
        æ–°é—»æ­£æ–‡ï¼š{news['news_text']}
        æ¶‰åŠå…¬å¸ï¼š{news['company_involved']}
        
        è¯·æä¾›ï¼š
        1. ä¸€å¥è¯æ–°é—»æ‘˜è¦
        2. å½±å“æ–¹å‘è¯„ä¼°ï¼š
           - è¾“å‡º+1è¡¨ç¤ºæ­£é¢å½±å“
           - è¾“å‡º-1è¡¨ç¤ºè´Ÿé¢å½±å“
           - è¾“å‡º0è¡¨ç¤ºä¸­æ€§å½±å“
        
        è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹ï¼š
        {{"summary": "è¿™é‡Œæ˜¯æ–°é—»æ‘˜è¦", "impact": å½±å“æ–¹å‘æ•°å­—}}
        """
        
        try:
            response = llm.invoke(prompt)
            # ä»response.contentä¸­æå–JSONå­—ç¬¦ä¸²
            content = response.content
            # æŸ¥æ‰¾JSONå¼€å§‹å’Œç»“æŸçš„ä½ç½®
            start = content.find('{')
            end = content.rfind('}') + 1
            if start != -1 and end != -1:
                json_str = content[start:end]
                # ç§»é™¤JSONä¸­æ•°å­—å‰çš„+å·ï¼Œå› ä¸ºè¿™ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼
                json_str = json_str.replace('"+1"', '1').replace('"+0"', '0').replace('"+', '')
                analysis = json.loads(json_str)
                factor_data.append({
                    "company_name": news["company_involved"],
                    "stock_code": news["stock_code"],
                    "stock_short_name": news["stock_short_name"],
                    "news_time": news["news_time"],
                    "news_title": news["news_title"],
                    "impact_direction": analysis["impact"],
                    "news_summary": analysis["summary"]
                })
                print(f"æˆåŠŸåˆ†ææ–°é—»: {news['news_title']}")
            else:
                print(f"æ— æ³•ä»å“åº”ä¸­æå–JSON: {content}")
        except Exception as e:
            print(f"åˆ†ææ–°é—»æ—¶å‡ºé”™: {str(e)}")
            print(f"é”™è¯¯çš„å“åº”å†…å®¹: {response.content if 'response' in locals() else 'No response'}")
            continue
            
    return factor_data

class SaveInput(BaseModel):
    data: List[Dict[str, Any]] = Field(..., description="è¦ä¿å­˜çš„æ•°æ®")
    filename: str = Field(default="factor.csv", description="è¾“å‡ºæ–‡ä»¶å")

@tool
def save_factor_data(input_data: SaveInput) -> str:
    """ä¿å­˜å› å­æ•°æ®åˆ°CSVæ–‡ä»¶"""
    if not input_data.data:
        return "No data to save"

    fieldnames = ['å…¬å¸å', 'è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨ç®€ç§°', 'æ–°é—»æ—¶é—´', 'æ–°é—»æ ‡é¢˜', 'å½±å“æ–¹å‘', 'æ–°é—»æ‘˜è¦']
    field_mapping = {
        'company_name': 'å…¬å¸å',
        'stock_code': 'è‚¡ç¥¨ä»£ç ',
        'stock_short_name': 'è‚¡ç¥¨ç®€ç§°',
        'news_time': 'æ–°é—»æ—¶é—´',
        'news_title': 'æ–°é—»æ ‡é¢˜',
        'impact_direction': 'å½±å“æ–¹å‘',
        'news_summary': 'æ–°é—»æ‘˜è¦'
    }
    
    try:
        with open(input_data.filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            for item in input_data.data:
                row = {field_mapping[k]: item[k] for k in field_mapping}
                writer.writerow(row)
        return f"Successfully saved {len(input_data.data)} records to {input_data.filename}"
    except Exception as e:
        return f"Error saving to CSV: {str(e)}"

@tool
def get_news_url(input_data: URLInput) -> str:
    """ä¸ç”¨æˆ·å¯¹è¯ï¼Œæ ¹æ®ç”¨æˆ·éœ€æ±‚æ¨èåˆé€‚çš„æ–°é—»URL"""
    # å¦‚æœè¾“å…¥å·²ç»æ˜¯URLï¼Œç›´æ¥è¿”å›
    if input_data.user_query.startswith('http'):
        return input_data.user_query
    # å¦åˆ™è¿”å›é»˜è®¤URL
    return "https://kuaixun.eastmoney.com/ssgs.html"

# èŠ‚ç‚¹å‡½æ•°
async def crawl_node(state: AgentState) -> AgentState:
    """æ–°é—»çˆ¬å–èŠ‚ç‚¹"""
    url = state["url"]
    news_data = await crawl_and_save_news.ainvoke(url)
    state["news_data"] = news_data
    return state

async def analyze_node(state: AgentState) -> AgentState:
    """æ–°é—»åˆ†æèŠ‚ç‚¹"""
    news_input = NewsInput(news_data=state["news_data"])
    factor_data = await analyze_news_impact.ainvoke({"input_data": news_input.model_dump()})
    state["factor_data"] = factor_data
    print("analyze_node")
    return state

async def save_node(state: AgentState) -> AgentState:
    """æ•°æ®ä¿å­˜èŠ‚ç‚¹"""
    input_data = SaveInput(data=state["factor_data"])
    await save_factor_data.ainvoke({"input_data": input_data.model_dump()})
    print("save_node")
    return state

async def get_url_node(state: AgentState) -> AgentState:
    """URLè·å–èŠ‚ç‚¹"""
    url_input = URLInput(user_query="æˆ‘éœ€è¦æœ€æ–°çš„ä¸Šå¸‚å…¬å¸ç›¸å…³æ–°é—»")
    url = await get_news_url.ainvoke({"input_data": url_input.model_dump()})
    state["url"] = url
    print(f"Selected URL based on user query: {url}")
    return state

# å·¥ä½œæµè®¾ç½®
workflow = StateGraph(
    AgentState,  # ä½¿ç”¨æˆ‘ä»¬å®šä¹‰çš„ AgentState ä½œä¸ºçŠ¶æ€æ¨¡å¼
)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("get_url", get_url_node)
workflow.add_node("crawl", crawl_node)
workflow.add_node("analyze", analyze_node)
workflow.add_node("save", save_node)

# æ·»åŠ è¾¹
workflow.add_edge("get_url", "crawl")
workflow.add_edge("crawl", "analyze")
workflow.add_edge("analyze", "save")
workflow.add_edge("save", END)

# è®¾ç½®å…¥å£
workflow.set_entry_point("get_url")

# ç¼–è¯‘å·¥ä½œæµ
chain = workflow.compile()

# è¿è¡Œå·¥ä½œæµ
async def run_workflow(user_query: str="æˆ‘æƒ³äº†è§£é‡‘èè´¢ç»çš„æœ€æ–°æ–°é—»"):
    # åˆå§‹åŒ–å¯¹è¯æ¨¡å‹
    llm = ChatTongyi(model="qwen-plus", api_key=os.getenv("DASHSCOPE_API_KEY"))
    
    # åˆå§‹åŒ–å¯¹è¯å†å²
    conversation_history = []
    system_prompt = SystemMessage(content="""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èæ–°é—»åŠ©æ‰‹ã€‚å½“ç”¨æˆ·è¯¢é—®æ–°é—»æ—¶ï¼Œè¯·ä»ä»¥ä¸‹URLä¸­é€‰æ‹©æœ€åˆé€‚çš„æ¨èç»™ç”¨æˆ·ï¼š
    1. https://kuaixun.eastmoney.com/ssgs.html (ä¸œæ–¹è´¢å¯Œä¸Šå¸‚å…¬å¸å¿«è®¯)

    
    è¯·ç›´æ¥åœ¨å›ç­”ä¸­åŒ…å«é€‰æ‹©çš„URLã€‚å¯¹äºå…¶ä»–é—®é¢˜ï¼Œè¯·æ­£å¸¸å›ç­”ã€‚
    """)
    conversation_history.append(system_prompt)
    
    print("æ¬¢è¿ä½¿ç”¨é‡‘èæ–°é—»åŠ©æ‰‹ï¼è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆè¾“å…¥'exit'é€€å‡ºï¼‰ï¼š")
    
    while True:
        user_input = input("\nğŸ‘¤ ç”¨æˆ·: ")
        if user_input.lower() == 'exit':
            break
            
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å¯¹è¯å†å²
        user_message = HumanMessage(content=user_input)
        conversation_history.append(user_message)
        
        # è·å–AIå“åº”
        response = llm.invoke(conversation_history)
        print(f"\nğŸ¤– åŠ©æ‰‹: {response.content}")
        conversation_history.append(response)
        
        # æ£€æŸ¥å“åº”ä¸­æ˜¯å¦åŒ…å«URL
        urls = re.findall(r'https?://[^\s<>"]+|www\.[^\s<>"]+', response.content)
        
        if urls:
            print("\næ£€æµ‹åˆ°æ–°é—»URLï¼Œæ˜¯å¦è¦æ‰§è¡Œæ–°é—»çˆ¬å–å’Œåˆ†æä»»åŠ¡ï¼Ÿ(y/n)")
            should_execute = input().lower()
            
            if should_execute == 'y':
                print("\nå¼€å§‹æ‰§è¡Œæ–°é—»çˆ¬å–å’Œåˆ†æä»»åŠ¡...")
                initial_state: AgentState = {
                    "messages": [],
                    "news_data": [],
                    "factor_data": [],
                    "url": urls[0]  # ä½¿ç”¨æ£€æµ‹åˆ°çš„ç¬¬ä¸€ä¸ªURL
                }
                
                try:
                    result = await chain.ainvoke(initial_state)
                    print("\nâœ… ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°CSVæ–‡ä»¶ä¸­ã€‚")
                except Exception as e:
                    print(f"\nâŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼š{str(e)}")
                    
    print("\næ„Ÿè°¢ä½¿ç”¨é‡‘èæ–°é—»åŠ©æ‰‹ï¼å†è§ï¼")

if __name__ == "__main__":
    # è¿è¡Œå¯¹è¯å¼å·¥ä½œæµ
    asyncio.run(run_workflow()) 